"""
–¢—Ä–µ–Ω–µ—Ä –¥–ª—è —ñ–Ω–≤–µ—Ä—Å—ñ–π–Ω–æ-—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ç–æ—Ä–Ω–æ–≥–æ —è–¥—Ä–∞.
"""
import json
import os
from dataclasses import asdict, dataclass
from datetime import datetime
from typing import Callable, Optional

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

from core.adaptive_controller import AdaptiveInversionController
from core.inversion_transformer import InversionTransformerCore
from core.perturbations import get_perturbation


@dataclass
class TrainConfig:
    """
    –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ.

    –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–∞–Ω–∏—Ö:
        data_mode (str): –†–µ–∂–∏–º –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –¥–∞–Ω–∏—Ö ('finance', 'sine', 'anomalies')
        n_samples (int): –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∑—Ä–∞–∑–∫—ñ–≤
        seq_len (int): –î–æ–≤–∂–∏–Ω–∞ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ
        input_dim (int): –†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å –≤—Ö—ñ–¥–Ω–∏—Ö –æ–∑–Ω–∞–∫
        train_split (float): –ß–∞—Å—Ç–∫–∞ –¥–∞–Ω–∏—Ö –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è (default: 0.8)

    –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –º–æ–¥–µ–ª—ñ:
        d_model (int): –†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å –º–æ–¥–µ–ª—ñ (default: 64)
        num_layers (int): –ö—ñ–ª—å–∫—ñ—Å—Ç—å transformer —à–∞—Ä—ñ–≤ (default: 2)
        num_heads (int): –ö—ñ–ª—å–∫—ñ—Å—Ç—å attention heads (default: 4)
        d_ff (int): –†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å feedforward –º–µ—Ä–µ–∂—ñ (default: 256)
        dropout (float): Dropout rate (default: 0.1)
        output_dim (int): –†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å –≤–∏—Ö–æ–¥—É (default: 1)
        use_representation_for_inv (bool): –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü—ñ—ó –¥–ª—è —ñ–Ω–≤–µ—Ä—Å—ñ—ó (default: False)

    –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –Ω–∞–≤—á–∞–Ω–Ω—è:
        batch_size (int): –†–æ–∑–º—ñ—Ä –±–∞—Ç—á–∞ (default: 32)
        num_epochs (int): –ö—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ø–æ—Ö (default: 50)
        learning_rate (float): Learning rate (default: 1e-3)
        weight_decay (float): Weight decay –¥–ª—è –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä–∞ (default: 1e-5)

    –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ —ñ–Ω–≤–µ—Ä—Å—ñ—ó:
        use_inversion (bool): –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ —ñ–Ω–≤–µ—Ä—Å—ñ–π–Ω—É —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—é (default: False)
        inv_weight (float): –í–∞–≥–∞ —ñ–Ω–≤–µ—Ä—Å—ñ–π–Ω–æ—ó –≤—Ç—Ä–∞—Ç–∏ (default: 0.1)
        perturbation_mode (str): –†–µ–∂–∏–º –∑–±—É—Ä–µ–Ω—å ('gaussian', 'timestep_dropout') (default: 'gaussian')
        perturbation_std (float): Std –¥–ª—è gaussian noise (default: 0.01)

    –Ü–Ω—à—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏:
        seed (int): Random seed (default: 42)
        results_root (str): –ö–æ—Ä–µ–Ω–µ–≤–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ (default: 'results')
        device (str): –ü—Ä–∏—Å—Ç—Ä—ñ–π ('cuda', 'cpu', 'auto') (default: 'auto')
    """

    # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–∞–Ω–∏—Ö
    data_mode: str = "finance"
    n_samples: int = 1000
    seq_len: int = 50
    input_dim: int = 1
    train_split: float = 0.8

    # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –º–æ–¥–µ–ª—ñ
    d_model: int = 64
    num_layers: int = 2
    num_heads: int = 4
    d_ff: int = 256
    dropout: float = 0.1
    output_dim: int = 1
    use_representation_for_inv: bool = False

    # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –Ω–∞–≤—á–∞–Ω–Ω—è
    batch_size: int = 32
    num_epochs: int = 50
    learning_rate: float = 1e-3
    weight_decay: float = 1e-5

    # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ —ñ–Ω–≤–µ—Ä—Å—ñ—ó
    use_inversion: bool = False
    inv_weight: float = 0.1
    perturbation_mode: str = "gaussian"
    perturbation_std: float = 0.01

    # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ—ó —ñ–Ω–≤–µ—Ä—Å—ñ—ó
    adaptive_mode: str = "off"  # {'off', 'static', 'online'}
    adaptive_warmup: int = 5  # –ö—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ø–æ—Ö warmup –ø–µ—Ä–µ–¥ online –∞–¥–∞–ø—Ç–∞—Ü—ñ—î—é
    adaptive_step: int = 3  # –ö—Ä–æ–∫ –∞–¥–∞–ø—Ç–∞—Ü—ñ—ó (–∫–æ–∂–Ω—ñ N –µ–ø–æ—Ö)
    adaptive_eta: float = 0.2  # Learning rate –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü—ñ—ó inv_weight
    adaptive_eta2: float = 0.1  # Learning rate –¥–ª—è val_loss gradient
    adaptive_r_target: float = 0.15  # –¶—ñ–ª—å–æ–≤–µ —Å–ø—ñ–≤–≤—ñ–¥–Ω–æ—à–µ–Ω–Ω—è inv_loss/base_loss
    inv_weight_min: float = 0.0  # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–π inv_weight
    inv_weight_max: float = 1.2  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∏–π inv_weight
    trend_correction_alpha: float = 0.35  # –ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç –∫–æ—Ä–µ–∫—Ü—ñ—ó —Ç—Ä–µ–Ω–¥—É
    deterministic_flat_thr: float = 0.15  # –ü–æ—Ä—ñ–≥ spectral_flatness
    deterministic_sent_thr: float = 0.3  # –ü–æ—Ä—ñ–≥ sample_entropy
    deterministic_guard_mode: str = "soft"  # {'soft', 'hard'}
    ema_alpha: float = 0.9  # EMA –∑–≥–ª–∞–¥–∂—É–≤–∞–Ω–Ω—è –¥–ª—è loss tracking

    # –Ü–Ω—à—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
    seed: int = 42
    results_root: str = "results"
    device: str = "auto"


class InversionTrainer:
    """
    –¢—Ä–µ–Ω–µ—Ä –¥–ª—è —ñ–Ω–≤–µ—Ä—Å—ñ–π–Ω–æ-—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ç–æ—Ä–Ω–æ–≥–æ —è–¥—Ä–∞.
    """

    def __init__(self, cfg: TrainConfig):
        """
        –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —Ç—Ä–µ–Ω–µ—Ä–∞.

        Args:
            cfg (TrainConfig): –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
        """
        self.cfg = cfg

        # –í—Å—Ç–∞–Ω–æ–≤–∏—Ç–∏ random seed
        torch.manual_seed(cfg.seed)
        np.random.seed(cfg.seed)

        # –í–∏–∑–Ω–∞—á–∏—Ç–∏ –ø—Ä–∏—Å—Ç—Ä—ñ–π
        if cfg.device == "auto":
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = torch.device(cfg.device)

        print(f"–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –ø—Ä–∏—Å—Ç—Ä—ñ–π: {self.device}")

        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –º–æ–¥–µ–ª—ñ
        self.model = InversionTransformerCore(
            input_dim=cfg.input_dim,
            d_model=cfg.d_model,
            num_layers=cfg.num_layers,
            num_heads=cfg.num_heads,
            d_ff=cfg.d_ff,
            dropout=cfg.dropout,
            output_dim=cfg.output_dim,
            use_representation_for_inv=cfg.use_representation_for_inv,
        ).to(self.device)

        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä–∞
        self.optimizer = optim.Adam(
            self.model.parameters(), lr=cfg.learning_rate, weight_decay=cfg.weight_decay
        )

        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∫—Ä–∏—Ç–µ—Ä—ñ—é –≤—Ç—Ä–∞—Ç–∏
        self.criterion = nn.MSELoss()

        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó –∑–±—É—Ä–µ–Ω—å
        if cfg.use_inversion:
            self.perturbation = get_perturbation(cfg.perturbation_mode, std=cfg.perturbation_std)
        else:
            self.perturbation = None

        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª–µ—Ä–∞
        if cfg.adaptive_mode in ["static", "online"]:
            self.adaptive_controller = AdaptiveInversionController(
                inv_weight_min=cfg.inv_weight_min,
                inv_weight_max=cfg.inv_weight_max,
                trend_correction_alpha=cfg.trend_correction_alpha,
                deterministic_flat_thr=cfg.deterministic_flat_thr,
                deterministic_sent_thr=cfg.deterministic_sent_thr,
                deterministic_guard_mode=cfg.deterministic_guard_mode,
                verbose=True,
            )
        else:
            self.adaptive_controller = None

    def train_once(
        self,
        X_train: np.ndarray,
        Y_train: np.ndarray,
        X_val: np.ndarray,
        Y_val: np.ndarray,
        callback: Optional[Callable] = None,
        tag: str = "default",
    ) -> dict:
        """
        –ü–æ–≤–Ω–∏–π —Ü–∏–∫–ª —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –∑ –ª–æ–≥—É–≤–∞–Ω–Ω—è–º.

        Args:
            X_train (np.ndarray): –¢—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ (n_samples, seq_len, input_dim)
            Y_train (np.ndarray): –¢—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ —Ç–∞—Ä–≥–µ—Ç–∏ (n_samples,)
            X_val (np.ndarray): –í–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω—ñ –¥–∞–Ω—ñ
            Y_val (np.ndarray): –í–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω—ñ —Ç–∞—Ä–≥–µ—Ç–∏
            callback (Optional[Callable]): Callback —Ñ—É–Ω–∫—Ü—ñ—è –¥–ª—è –∫–æ–∂–Ω–æ—ó –µ–ø–æ—Ö–∏
            tag (str): –¢–µ–≥ –¥–ª—è run (default: 'default')

        Returns:
            Dict: –°–ª–æ–≤–Ω–∏–∫ –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
        """
        # –°—Ç–≤–æ—Ä–∏—Ç–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—é –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
        run_dir = self._generate_run_dir(tag)
        os.makedirs(run_dir, exist_ok=True)

        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –±—É–¥—É—Ç—å –∑–±–µ—Ä–µ–∂–µ–Ω—ñ –≤: {run_dir}")

        # –ö–æ–Ω–≤–µ—Ä—Ç—É–≤–∞—Ç–∏ –≤ torch tensors
        X_train_t = torch.FloatTensor(X_train).to(self.device)
        Y_train_t = torch.FloatTensor(Y_train).unsqueeze(1).to(self.device)
        X_val_t = torch.FloatTensor(X_val).to(self.device)
        Y_val_t = torch.FloatTensor(Y_val).unsqueeze(1).to(self.device)

        # –°—Ç–≤–æ—Ä–∏—Ç–∏ DataLoader
        train_dataset = TensorDataset(X_train_t, Y_train_t)
        train_loader = DataLoader(train_dataset, batch_size=self.cfg.batch_size, shuffle=True)

        # –í—ñ–¥–∫—Ä–∏—Ç–∏ —Ñ–∞–π–ª –¥–ª—è –ª–æ–≥—É–≤–∞–Ω–Ω—è –º–µ—Ç—Ä–∏–∫ –ø–æ –µ–ø–æ—Ö–∞—Ö
        metrics_file = os.path.join(run_dir, "epoch_metrics.jsonl")

        best_val_loss = float("inf")
        train_history = []

        # –ê–¥–∞–ø—Ç–∏–≤–Ω–∞ —ñ–Ω–≤–µ—Ä—Å—ñ—è: —Å—Ç–∞—Ç–∏—á–Ω–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è
        if self.cfg.adaptive_mode == "static" and self.adaptive_controller is not None:
            print("\n" + "=" * 70)
            print("üîç ADAPTIVE INVERSION: Static recommendation")
            print("=" * 70)
            recommended_weight, explanation = self.adaptive_controller.recommend(X_train, Y_train)
            self.cfg.inv_weight = recommended_weight
            print(f"–†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–∏–π inv_weight: {recommended_weight:.3f}")
            print(explanation)
            print("=" * 70 + "\n")

        # Online –∞–¥–∞–ø—Ç–∞—Ü—ñ—è: —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è EMA
        current_inv_weight = self.cfg.inv_weight
        inv_weight_history = [current_inv_weight]

        if self.cfg.adaptive_mode == "online" and self.adaptive_controller is not None:
            # –û—Ç—Ä–∏–º–∞—Ç–∏ –ø–æ—á–∞—Ç–∫–æ–≤—É —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—é
            print("\n" + "=" * 70)
            print("üîç ADAPTIVE INVERSION: Online mode - initial recommendation")
            print("=" * 70)
            recommended_weight, explanation = self.adaptive_controller.recommend(X_train, Y_train)
            current_inv_weight = recommended_weight
            inv_weight_history[0] = current_inv_weight
            print(f"–ü–æ—á–∞—Ç–∫–æ–≤–∏–π inv_weight: {current_inv_weight:.3f}")
            print(explanation)
            print("=" * 70 + "\n")

            # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è EMA
            base_loss_ema = None
            inv_loss_ema = None
            val_loss_ema = None
            prev_val_loss_ema = None

        # –¢—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏–π —Ü–∏–∫–ª
        for epoch in range(self.cfg.num_epochs):
            self.model.train()

            epoch_base_loss = 0.0
            epoch_inv_loss = 0.0
            epoch_total_loss = 0.0
            num_batches = 0

            for batch_X, batch_Y in train_loader:
                self.optimizer.zero_grad()

                # Forward pass
                y_pred, h = self.model(batch_X, return_repr=self.cfg.use_representation_for_inv)

                # –ë–∞–∑–æ–≤–∞ –≤—Ç—Ä–∞—Ç–∞
                base_loss = self.criterion(y_pred, batch_Y)

                # –Ü–Ω–≤–µ—Ä—Å—ñ–π–Ω–∞ –≤—Ç—Ä–∞—Ç–∞
                inv_loss = torch.tensor(0.0, device=self.device)
                if self.cfg.use_inversion and self.perturbation is not None:
                    # –°—Ç–≤–æ—Ä–∏—Ç–∏ –∑–±—É—Ä–µ–Ω–∏–π –≤—Ö—ñ–¥
                    batch_X_perturbed = self.perturbation.apply(batch_X)

                    # Forward pass –Ω–∞ –∑–±—É—Ä–µ–Ω–æ–º—É –≤—Ö–æ–¥—ñ
                    y_pred_perturbed, h_perturbed = self.model(
                        batch_X_perturbed, return_repr=self.cfg.use_representation_for_inv
                    )

                    # –û–±—á–∏—Å–ª–∏—Ç–∏ —ñ–Ω–≤–µ—Ä—Å—ñ–π–Ω—É –≤—Ç—Ä–∞—Ç—É
                    if self.cfg.use_representation_for_inv:
                        # –Ü–Ω–≤–µ—Ä—Å—ñ—è –Ω–∞ —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü—ñ—è—Ö
                        inv_loss = self.criterion(h_perturbed, h.detach())
                    else:
                        # –Ü–Ω–≤–µ—Ä—Å—ñ—è –Ω–∞ –≤–∏—Ö–æ–¥–∞—Ö
                        inv_loss = self.criterion(y_pred_perturbed, y_pred.detach())

                # –ó–∞–≥–∞–ª—å–Ω–∞ –≤—Ç—Ä–∞—Ç–∞ (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –ø–æ—Ç–æ—á–Ω–∏–π –¥–∏–Ω–∞–º—ñ—á–Ω–∏–π inv_weight)
                total_loss = base_loss + current_inv_weight * inv_loss

                # Backward pass
                total_loss.backward()
                self.optimizer.step()

                # –õ–æ–≥—É–≤–∞–Ω–Ω—è
                epoch_base_loss += base_loss.item()
                epoch_inv_loss += inv_loss.item()
                epoch_total_loss += total_loss.item()
                num_batches += 1

            # –£—Å–µ—Ä–µ–¥–Ω–∏—Ç–∏ –≤—Ç—Ä–∞—Ç–∏
            epoch_base_loss /= num_batches
            epoch_inv_loss /= num_batches
            epoch_total_loss /= num_batches

            # –í–∞–ª—ñ–¥–∞—Ü—ñ—è
            self.model.eval()
            with torch.no_grad():
                val_pred, _ = self.model(X_val_t, return_repr=False)
                val_loss = self.criterion(val_pred, Y_val_t).item()

            # –û–±—á–∏—Å–ª–∏—Ç–∏ –º–µ—Ç—Ä–∏–∫–∏
            gain = (epoch_base_loss - val_loss) / (epoch_base_loss + 1e-8)
            inv_eff = epoch_inv_loss / (epoch_base_loss + 1e-8) if self.cfg.use_inversion else 0.0

            # Online –∞–¥–∞–ø—Ç–∞—Ü—ñ—è
            if self.cfg.adaptive_mode == "online" and self.adaptive_controller is not None:
                # –û–Ω–æ–≤–∏—Ç–∏ EMA
                if base_loss_ema is None:
                    base_loss_ema = epoch_base_loss
                    inv_loss_ema = epoch_inv_loss
                    val_loss_ema = val_loss
                    prev_val_loss_ema = val_loss
                else:
                    prev_val_loss_ema = val_loss_ema
                    base_loss_ema = (
                        self.cfg.ema_alpha * base_loss_ema
                        + (1 - self.cfg.ema_alpha) * epoch_base_loss
                    )
                    inv_loss_ema = (
                        self.cfg.ema_alpha * inv_loss_ema
                        + (1 - self.cfg.ema_alpha) * epoch_inv_loss
                    )
                    val_loss_ema = (
                        self.cfg.ema_alpha * val_loss_ema + (1 - self.cfg.ema_alpha) * val_loss
                    )

                # –ó–∞—Å—Ç–æ—Å—É–≤–∞—Ç–∏ –∞–¥–∞–ø—Ç–∞—Ü—ñ—é –ø—ñ—Å–ª—è warmup –∫–æ–∂–Ω—ñ adaptive_step –µ–ø–æ—Ö
                if (
                    epoch >= self.cfg.adaptive_warmup
                    and (epoch - self.cfg.adaptive_warmup) % self.cfg.adaptive_step == 0
                ):
                    new_weight, adaptation_reason = self.adaptive_controller.update_with_feedback(
                        epoch=epoch + 1,
                        base_loss_ema=base_loss_ema,
                        inv_loss_ema=inv_loss_ema,
                        val_loss_ema=val_loss_ema,
                        prev_val_loss_ema=prev_val_loss_ema,
                        current_inv_weight=current_inv_weight,
                        adaptive_eta=self.cfg.adaptive_eta,
                        adaptive_eta2=self.cfg.adaptive_eta2,
                        r_target=self.cfg.adaptive_r_target,
                    )

                    if abs(new_weight - current_inv_weight) > 0.01:
                        print(f"\nüìä Epoch {epoch+1}: Adaptive update")
                        print(f"   inv_weight: {current_inv_weight:.3f} ‚Üí {new_weight:.3f}")
                        print(
                            f"   Reason: r={inv_loss_ema/(base_loss_ema+1e-12):.4f}, val_trend={val_loss_ema-prev_val_loss_ema:.6f}"
                        )

                    current_inv_weight = new_weight

                inv_weight_history.append(current_inv_weight)
            else:
                inv_weight_history.append(current_inv_weight)

            # –õ–æ–≥—É–≤–∞–Ω–Ω—è –º–µ—Ç—Ä–∏–∫
            epoch_metrics = {
                "epoch": epoch + 1,
                "base_loss": epoch_base_loss,
                "inv_loss": epoch_inv_loss,
                "total_loss": epoch_total_loss,
                "val_loss": val_loss,
                "gain": gain,
                "inv_eff": inv_eff,
                "inv_weight": float(current_inv_weight),
            }

            # –î–æ–¥–∞—Ç–∏ EMA –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è online —Ä–µ–∂–∏–º—É
            if self.cfg.adaptive_mode == "online" and base_loss_ema is not None:
                epoch_metrics["base_loss_ema"] = float(base_loss_ema)
                epoch_metrics["inv_loss_ema"] = float(inv_loss_ema)
                epoch_metrics["val_loss_ema"] = float(val_loss_ema)
                epoch_metrics["ratio_r"] = float(inv_loss_ema / (base_loss_ema + 1e-12))

            # –ó–±–µ—Ä–µ–≥—Ç–∏ –≤ —Ñ–∞–π–ª
            with open(metrics_file, "a") as f:
                f.write(json.dumps(epoch_metrics) + "\n")

            train_history.append(epoch_metrics)

            # Callback
            if callback is not None:
                callback(epoch, epoch_metrics)

            # –í–∏–≤–µ—Å—Ç–∏ –ø—Ä–æ–≥—Ä–µ—Å
            if (epoch + 1) % 10 == 0 or epoch == 0:
                print(
                    f"Epoch {epoch+1}/{self.cfg.num_epochs}: "
                    f"base_loss={epoch_base_loss:.4f}, "
                    f"inv_loss={epoch_inv_loss:.4f}, "
                    f"val_loss={val_loss:.4f}"
                )

            # –ó–±–µ—Ä–µ–≥—Ç–∏ –Ω–∞–π–∫—Ä–∞—â—É –º–æ–¥–µ–ª—å
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                torch.save(self.model.state_dict(), os.path.join(run_dir, "best_model.pt"))

        # –ì–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—é inv_weight schedule
        self._plot_inv_weight_schedule(inv_weight_history, train_history, run_dir)

        # –ó–±–µ—Ä–µ–≥—Ç–∏ —Ñ—ñ–Ω–∞–ª—å–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ run
        run_info = {
            "config": asdict(self.cfg),
            "model_info": self.model.model_info(),
            "best_val_loss": best_val_loss,
            "final_train_loss": epoch_base_loss,
            "run_dir": run_dir,
            "tag": tag,
            "inv_weight_history": [float(w) for w in inv_weight_history],
        }

        with open(os.path.join(run_dir, "run_info.json"), "w") as f:
            json.dump(run_info, f, indent=2)

        print(f"\n–¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –ù–∞–π–∫—Ä–∞—â–∞ val_loss: {best_val_loss:.4f}")

        # –í–∏–≤–µ—Å—Ç–∏ –ø–æ—è—Å–Ω–µ–Ω–Ω—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª–µ—Ä–∞
        if self.adaptive_controller is not None:
            explanation = self.adaptive_controller.explain()
            explanation_file = os.path.join(run_dir, "adaptive_explanation.txt")
            with open(explanation_file, "w", encoding="utf-8") as f:
                f.write(explanation)
            print(f"\n–ê–¥–∞–ø—Ç–∏–≤–Ω–µ –ø–æ—è—Å–Ω–µ–Ω–Ω—è –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤: {explanation_file}")

        return run_info

    def evaluate(
        self,
        X_test: np.ndarray,
        Y_test: np.ndarray,
        noise_levels: list[float] = [0.0, 0.01, 0.05, 0.1],
    ) -> dict:
        """
        –û—Ü—ñ–Ω–∫–∞ —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—ñ –º–æ–¥–µ–ª—ñ –¥–æ —Ä—ñ–∑–Ω–∏—Ö —Ä—ñ–≤–Ω—ñ–≤ —à—É–º—É.

        Args:
            X_test (np.ndarray): –¢–µ—Å—Ç–æ–≤—ñ –¥–∞–Ω—ñ
            Y_test (np.ndarray): –¢–µ—Å—Ç–æ–≤—ñ —Ç–∞—Ä–≥–µ—Ç–∏
            noise_levels (List[float]): –†—ñ–≤–Ω—ñ —à—É–º—É –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è

        Returns:
            Dict: –°–ª–æ–≤–Ω–∏–∫ –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ—Ü—ñ–Ω–∫–∏
        """
        self.model.eval()
        results = {}

        X_test_t = torch.FloatTensor(X_test).to(self.device)
        Y_test_t = torch.FloatTensor(Y_test).unsqueeze(1).to(self.device)

        with torch.no_grad():
            for noise_level in noise_levels:
                # –î–æ–¥–∞—Ç–∏ —à—É–º
                if noise_level > 0:
                    noise = torch.randn_like(X_test_t) * noise_level
                    X_noisy = X_test_t + noise
                else:
                    X_noisy = X_test_t

                # –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
                y_pred, _ = self.model(X_noisy, return_repr=False)

                # –û–±—á–∏—Å–ª–∏—Ç–∏ MSE
                mse = self.criterion(y_pred, Y_test_t).item()

                results[f"noise_{noise_level}"] = {"mse": mse, "rmse": np.sqrt(mse)}

        return results

    def _plot_inv_weight_schedule(
        self, inv_weight_history: list[float], train_history: list[dict], run_dir: str
    ):
        """
        –°—Ç–≤–æ—Ä—é—î –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—é –∑–º—ñ–Ω–∏ inv_weight –ø–æ –µ–ø–æ—Ö–∞—Ö.

        Args:
            inv_weight_history (List[float]): –Ü—Å—Ç–æ—Ä—ñ—è inv_weight
            train_history (List[Dict]): –Ü—Å—Ç–æ—Ä—ñ—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
            run_dir (str): –î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è
        """
        try:
            import matplotlib.pyplot as plt

            epochs = [m["epoch"] for m in train_history]
            val_losses = [m["val_loss"] for m in train_history]
            inv_losses = [m["inv_loss"] for m in train_history]

            # –ü–µ—Ä–µ–∫–æ–Ω–∞—Ç–∏—Å—è —â–æ inv_weight_history –º–∞—î –ø—Ä–∞–≤–∏–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä
            # –í—ñ–Ω –º–æ–∂–µ –º–∞—Ç–∏ –Ω–∞ 1 –µ–ª–µ–º–µ–Ω—Ç –±—ñ–ª—å—à–µ —á–µ—Ä–µ–∑ –ø–æ—á–∞—Ç–∫–æ–≤—É —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—é
            if len(inv_weight_history) > len(epochs):
                inv_weight_history = inv_weight_history[1:]  # –í–∏–¥–∞–ª–∏—Ç–∏ –ø–µ—Ä—à–∏–π –µ–ª–µ–º–µ–Ω—Ç
            elif len(inv_weight_history) < len(epochs):
                # –Ø–∫—â–æ –º–µ–Ω—à–µ, –¥–æ–¥–∞—Ç–∏ –æ—Å—Ç–∞–Ω–Ω—î –∑–Ω–∞—á–µ–Ω–Ω—è
                inv_weight_history = inv_weight_history + [inv_weight_history[-1]] * (
                    len(epochs) - len(inv_weight_history)
                )

            # –°—Ç–≤–æ—Ä–∏—Ç–∏ figure –∑ –¥–≤–æ–º–∞ subplots
            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), sharex=True)

            # Plot 1: inv_weight
            ax1.plot(epochs, inv_weight_history, "b-", linewidth=2, label="inv_weight")
            ax1.axhline(
                y=self.cfg.inv_weight_max,
                color="r",
                linestyle="--",
                alpha=0.5,
                label=f"max={self.cfg.inv_weight_max}",
            )
            ax1.axhline(
                y=self.cfg.inv_weight_min,
                color="g",
                linestyle="--",
                alpha=0.5,
                label=f"min={self.cfg.inv_weight_min}",
            )
            if self.cfg.adaptive_warmup > 0:
                ax1.axvline(
                    x=self.cfg.adaptive_warmup,
                    color="orange",
                    linestyle=":",
                    alpha=0.7,
                    label=f"warmup={self.cfg.adaptive_warmup}",
                )
            ax1.set_ylabel("inv_weight", fontsize=12)
            ax1.set_title(
                f"Adaptive Inversion Schedule ({self.cfg.adaptive_mode} mode)",
                fontsize=14,
                fontweight="bold",
            )
            ax1.legend(loc="upper right")
            ax1.grid(True, alpha=0.3)

            # Plot 2: Losses
            ax2_twin = ax2.twinx()
            ax2.plot(epochs, val_losses, "g-", linewidth=2, label="val_loss", alpha=0.8)
            ax2_twin.plot(epochs, inv_losses, "r-", linewidth=2, label="inv_loss", alpha=0.8)

            ax2.set_xlabel("Epoch", fontsize=12)
            ax2.set_ylabel("Validation Loss", fontsize=12, color="g")
            ax2_twin.set_ylabel("Inversion Loss", fontsize=12, color="r")
            ax2.tick_params(axis="y", labelcolor="g")
            ax2_twin.tick_params(axis="y", labelcolor="r")
            ax2.grid(True, alpha=0.3)

            # Combine legends
            lines1, labels1 = ax2.get_legend_handles_labels()
            lines2, labels2 = ax2_twin.get_legend_handles_labels()
            ax2.legend(lines1 + lines2, labels1 + labels2, loc="upper right")

            plt.tight_layout()

            save_path = os.path.join(run_dir, "inv_weight_schedule.png")
            plt.savefig(save_path, dpi=150, bbox_inches="tight")
            plt.close()

            print(f"üìä –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è inv_weight –∑–±–µ—Ä–µ–∂–µ–Ω–∞: {save_path}")

        except Exception as e:
            print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—ñ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó: {e}")

    def _generate_run_dir(self, tag: str) -> str:
        """
        –°—Ç–≤–æ—Ä—é—î –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—é –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ run.

        Args:
            tag (str): –¢–µ–≥ –¥–ª—è run

        Returns:
            str: –®–ª—è—Ö –¥–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        run_name = f"{self.cfg.data_mode}_{tag}_{timestamp}"
        run_dir = os.path.join(self.cfg.results_root, run_name)
        return run_dir
